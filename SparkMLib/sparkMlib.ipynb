{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "\r\n",
    "\r\n",
    "from pyspark.ml import Pipeline\r\n",
    "from pyspark.ml.classification import LogisticRegression\r\n",
    "from pyspark.ml.feature import HashingTF, Tokenizer\r\n",
    "from pyspark.sql import Row\r\n",
    "from pyspark.sql.functions import UserDefinedFunction\r\n",
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading data and computation of outcome column used as the dependent variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "df = spark.read.format(\"csv\").option(\"header\", \"true\").load(\"/data/results.csv\")\n",
    "\n",
    "df=df.withColumn('diff', col('home_score')-col('away_score'))\n",
    "df=df.withColumn('home_out',expr(\"\"\"IF(diff > 0, 'w', IF(diff < 0, 'l', 'd'))\"\"\")).withColumn('away_out',expr(\"\"\"IF(diff > 0, 'l', IF(diff < 0, 'w', 'd'))\"\"\"))\n",
    "#df.show()\n",
    "dfhome=df.select(\"home_team\",\"away_team\",\"city\",\"home_out\").withColumnRenamed(\"home_team\", \"team\").withColumnRenamed(\"away_team\", \"rival\").withColumnRenamed(\"home_out\", \"outcome\")\n",
    "dfaway=df.select(\"away_team\",\"home_team\",\"city\",\"away_out\").withColumnRenamed(\"away_team\", \"team\").withColumnRenamed(\"home_team\", \"rival\").withColumnRenamed(\"away_out\", \"outcome\")\n",
    "\n",
    "df_data = dfhome.unionByName(dfaway)\n",
    "df_data.show()\n",
    "print((df_data.count(), len(df_data.columns)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "####################################################################################################################################################\n",
    "\n",
    "# Create a LogisticRegression instance. This instance is an Estimator.\n",
    "lr = LogisticRegression(maxIter=10, regParam=0.01)\n",
    "\n",
    "# Print out the parameters, documentation, and any default values.\n",
    "print (\"LogisticRegression parameters\")\n",
    "\n",
    "#categorical data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import OneHotEncoderEstimator, StringIndexer, VectorAssembler\n",
    "categoricalColumns = ['team', 'rival', 'city']\n",
    "stages = []\n",
    "\n",
    "#encoding categorical data\n",
    "for categoricalCol in categoricalColumns:\n",
    "    stringIndexer = StringIndexer(inputCol = categoricalCol, outputCol = categoricalCol + 'Index')\n",
    "    encoder = OneHotEncoderEstimator(inputCols=[stringIndexer.getOutputCol()], outputCols=[categoricalCol + \"classVec\"])\n",
    "    stages += [stringIndexer, encoder]\n",
    "    \n",
    "label_stringIdx = StringIndexer(inputCol = 'outcome', outputCol = 'label')\n",
    "stages += [label_stringIdx]\n",
    "\n",
    "assemblerInputs = [c + \"classVec\" for c in categoricalColumns]\n",
    "assembler = VectorAssembler(inputCols=assemblerInputs, outputCol=\"features\")\n",
    "stages += [assembler]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "\r\n",
    "from pyspark.ml import Pipeline\r\n",
    "cols = df_data.columns #############\r\n",
    "\r\n",
    "pipeline = Pipeline(stages = stages)\r\n",
    "\r\n",
    "pipelineModel = pipeline.fit(df_data)\r\n",
    "df_data = pipelineModel.transform(df_data)\r\n",
    "selectedCols = ['label', 'features'] + cols\r\n",
    "df_data = df_data.select(selectedCols)\r\n",
    "df_data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.DataFrame(df_data.take(5), columns=df_data.columns).transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "\r\n",
    "train, test = df_data.randomSplit([0.8, 0.2], seed = 1800)\r\n",
    "print(\"Training Dataset Count: \" + str(train.count()))\r\n",
    "print(\"Test Dataset Count: \" + str(test.count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "\r\n",
    "train.show()\r\n",
    "test.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "lr = LogisticRegression(featuresCol = 'features', labelCol = 'label', maxIter=10)\n",
    "lrModel = lr.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "print (\"lrModel  was fit using parameters: \")\n",
    "print (lrModel.extractParamMap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "print(\"Multinomial coefficients: \" + str(lrModel.coefficientMatrix))\n",
    "print(\"Multinomial intercepts: \" + str(lrModel.interceptVector))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "# Extract the summary from the returned LogisticRegressionModel instance trained\n",
    "# in the earlier example\n",
    "trainingSummary = lrModel.summary\n",
    "\n",
    "# Obtain the objective per iteration\n",
    "objectiveHistory = trainingSummary.objectiveHistory\n",
    "\n",
    "print(\"False positive rate by label:\")\n",
    "for i, rate in enumerate(trainingSummary.falsePositiveRateByLabel):\n",
    "    print(\"label %d: %s\" % (i, rate))\n",
    "\n",
    "print(\"True positive rate by label:\")\n",
    "for i, rate in enumerate(trainingSummary.truePositiveRateByLabel):\n",
    "    print(\"label %d: %s\" % (i, rate))\n",
    "\n",
    "print(\"Precision by label:\")\n",
    "for i, prec in enumerate(trainingSummary.precisionByLabel):\n",
    "    print(\"label %d: %s\" % (i, prec))\n",
    "\n",
    "print(\"Recall by label:\")\n",
    "for i, rec in enumerate(trainingSummary.recallByLabel):\n",
    "    print(\"label %d: %s\" % (i, rec))\n",
    "\n",
    "print(\"F-measure by label:\")\n",
    "for i, f in enumerate(trainingSummary.fMeasureByLabel()):\n",
    "    print(\"label %d: %s\" % (i, f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "accuracy = trainingSummary.accuracy\n",
    "falsePositiveRate = trainingSummary.weightedFalsePositiveRate\n",
    "truePositiveRate = trainingSummary.weightedTruePositiveRate\n",
    "fMeasure = trainingSummary.weightedFMeasure()\n",
    "precision = trainingSummary.weightedPrecision\n",
    "recall = trainingSummary.weightedRecall\n",
    "print(\"Accuracy: %s\\nFPR: %s\\nTPR: %s\\nF-measure: %s\\nPrecision: %s\\nRecall: %s\"% (accuracy, falsePositiveRate, truePositiveRate, fMeasure, precision, recall))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "predictions = lrModel.transform(test)\n",
    "dr_predit=predictions.select('team','rival','city', 'label','rawPrediction', 'prediction', 'probability')\n",
    "dr_predit.show(10)\n",
    "#label_stringIdx = StringIndexer(inputCol = 'outcome', outputCol = 'label')\n",
    "#user_labels = user_indexer_model.labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import IndexToString\n",
    "print(stages)\n",
    "print(pipelineModel)\n",
    "#label_stringIdx = StringIndexer(inputCol = 'outcome', outputCol = 'label')\n",
    "\n",
    "ind_str = IndexToString(inputCol='prediction',outputCol='pred_label',labels=pipelineModel.stages[6].labels)\n",
    "\n",
    "dr_predit=dr_predit.withColumnRenamed(\"label\", \"enc_label\")\n",
    "predictions_dec = ind_str.transform(dr_predit)\n",
    "predictions_dec.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "df_with_prediction = StringIndexerModel.transform(dr_predit).withColumnRenamed(\n",
    "    \"label\", \"prediction\"\n",
    ")\n",
    "\n",
    "idx_to_string.transform(df_with_prediction).show()\n",
    "\n",
    "\n",
    "user_id_to_label = IndexToString(inputCol=\"prediction\", outputCol=\"predictionlabel\", labels=label_stringIdx)\n",
    "user_id_to_label.transform(dr_predit).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "evaluator = BinaryClassificationEvaluator()\n",
    "print('Test Area Under ROC', evaluator.evaluate(predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regression performed poorly because it is too weak given the range of different features. The prediction accuracy of decision trees can be improved by Ensemble methods, such as Random Forest and Gradient-Boosted Tree."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "rf = RandomForestClassifier(featuresCol = 'features', labelCol = 'label')\n",
    "rfModel = rf.fit(train)\n",
    "predictions = rfModel.transform(test)\n",
    "df_predit=predictions.select('team','rival','city', 'rawPrediction', 'prediction', 'probability')\n",
    "predictions_dec = ind_str.transform(df_predit)\n",
    "predictions_dec.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "evaluator = BinaryClassificationEvaluator()\n",
    "print(\"Test Area Under ROC: \" + str(evaluator.evaluate(predictions, {evaluator.metricName: \"areaUnderROC\"})))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test Area Under ROC: 0.6052702020100458"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "logistic regression model with elastic net regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "lr = LogisticRegression(maxIter=10, regParam=0.3, elasticNetParam=0.8)\n",
    "\n",
    "# Fit the model\n",
    "lrModel = lr.fit(train)\n",
    "\n",
    "# Print the coefficients and intercept for logistic regression\n",
    "print(\"Coefficients: \" + str(lrModel.coefficientMatrix))\n",
    "print(\"Intercept: \" + str(lrModel.interceptVector))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "from pyspark.ml.feature import StringIndexer, VectorIndexer\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "# Index labels, adding metadata to the label column.\n",
    "# Fit on whole dataset to include all labels in index.\n",
    "labelIndexer = StringIndexer(inputCol=\"label\", outputCol=\"indexedLabel\").fit(df_data)\n",
    "# Automatically identify categorical features, and index them.\n",
    "# We specify maxCategories so features with > 4 distinct values are treated as continuous.\n",
    "featureIndexer =VectorIndexer(inputCol=\"features\", outputCol=\"indexedFeatures\", maxCategories=4).fit(df_data)\n",
    "\n",
    "# Train a DecisionTree model.\n",
    "dt = DecisionTreeClassifier(labelCol=\"label\", featuresCol=\"features\")\n",
    "\n",
    "# Chain indexers and tree in a Pipeline\n",
    "pipeline = Pipeline(stages=[labelIndexer, featureIndexer, dt])\n",
    "\n",
    "# Train model.  This also runs the indexers.\n",
    "model = pipeline.fit(train)\n",
    "\n",
    "# Make predictions.\n",
    "predictions = model.transform(test)\n",
    "\n",
    "# Select example rows to display.\n",
    "df_predit=predictions.select(\"prediction\", \"indexedLabel\", \"features\")\n",
    "predictions_dec = ind_str.transform(df_predit)\n",
    "predictions_dec.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "# Select example rows to display.\n",
    "predictions.select(\"prediction\", \"indexedLabel\", \"features\")\n",
    "\n",
    "# Select (prediction, true label) and compute test error\n",
    "evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"indexedLabel\", predictionCol=\"prediction\", metricName=\"accuracy\")  #Precision\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print (\"Test Error = %g\" % (1.0 - accuracy))\n",
    "\n",
    "treeModel = model.stages[2]\n",
    "print (treeModel) # summary only"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Spark 2.0.0",
   "language": "python",
   "name": "spark2"
  },
  "language_info": {
   "codemirror_mode": "text/python",
   "file_extension": ".py",
   "mimetype": "text/python",
   "name": "scala",
   "pygments_lexer": "python",
   "version": "3.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
